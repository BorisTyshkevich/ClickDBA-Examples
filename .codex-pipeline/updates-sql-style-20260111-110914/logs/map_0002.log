OpenAI Codex v0.80.0 (research preview)
--------
workdir: /Users/bvt/work/ClickDBA/q1-fhir
model: gpt-5.2
provider: openai
approval: never
sandbox: workspace-write [workdir, /tmp, $TMPDIR]
reasoning effort: medium
reasoning summaries: auto
session id: 019bac88-3c99-7090-9543-e558344f2f6b
--------
user
You are reading a single chunk from a larger article.
Do NOT run any commands or tools.
Do NOT add filler. Be information-dense and practical.
If something is uncertain, say so briefly.

CHUNK 2/4:
-----
This was a practical stopgap, an easier, faster optimization that gave us breathing room to build something better. The real breakthrough was still ahead: patch parts , a fundamentally different mechanism designed to handle frequent updates efficiently at scale.

Stage 3: Patch parts – updates the ClickHouse way #

Earlier approaches weren’t good enough, so we built something better.

Why classic mutations weren’t enough #

Even with optimizations like on-the-fly updates, the core model had limits:

Rewrites of entire columns, even if just a few rows have changed, which is wasteful at scale.

Blocking behind previous merges and mutations, adding latency and unpredictability.

That’s why we built a new mechanism from scratch — patch parts — named for what they do: they patch source parts during merges, applying only the changed data.

A model built on what works #

Patch parts borrow two proven ideas from our specialized engines , fast inserts and background merges , and generalize them, fully encapsulated for flexible, SQL-style updates:

Fast inserts : ClickHouse handles inserts at high throughput (e.g., in one production setup, over ClickHouse ingests over 1 billion rows per second ). We use this to model updates and deletes as lightweight inserts.

Background merges : MergeTree is already scanning and rewriting data. Applying updates or deleting rows during this process adds near-zero overhead .

Let’s walk through a simple example to understand how this works. ( We’ll start small to build intuition, then go through the internals in the next section.)

How patch parts work #

Let’s understand how patch parts behave and why they’re so efficient.

We’ll reuse the small orders table from earlier to walk through a simple update:

1 CREATE TABLE orders ( 2 order_id Int32, 3 item_id String, 4 quantity UInt32, 5 price Decimal ( 10 , 2 ), 6 discount Decimal ( 5 , 2 ) 7 ) 8 ENGINE = MergeTree 9 ORDER BY (order_id, item_id);

UPDATEs powered by patch parts are still experimental in 25.7 
To try this out in ClickHouse 25.7, you’ll need to enable the feature manually:

SET allow_experimental_lightweight_update = 1; 
ALTER TABLE orders MODIFY SETTING enable_block_number_column = 1, enable_block_offset_column = 1;

(It is expected to enter beta in 25.8)

We’ll also reuse the same initial order from earlier: two items (kbd, mouse). These rows are inserted into a data part named all_1_1_0 (shown in a diagram further below):

1 INSERT INTO orders VALUES 2 ( 1001 , 'kbd ', 10 , 45.00 , 0.00 ), 3 ( 1001 , 'mouse ', 6 , 25.00 , 0.00 );

Let’s say the mouse quantity is later increased to 60 units, qualifying for a 20% bulk discount. We update the row:

1 UPDATE orders 2 SET quantity = 60 , discount = 0.20 3 WHERE order_id = 1001 AND item_id = 'mouse ';

This UPDATE triggers a lightweight update , powered by patch parts.

Patch part based updates use standard SQL UPDATE syntax . This new feature is called a lightweight update in ClickHouse. Unlike classic mutations, it behaves more like a row-level update: small, frequent changes are efficient and performant.

At this point, the updated values are already visible to queries , no need to wait for any background work to complete. We’ll explain how that works shortly .

A patch part is a delta, not a replacement #

Unlike classic mutations, ClickHouse doesn’t rewrite the entire column or part. Instead, it creates a new, compact patch part that contains only:

The changed column values (quantity = 60, discount = 0.20)

Metadata to locate the original row inside the source part

Think of a patch part as a “diff”: a small delta that says “update just this row, just these columns.” That’s why we call them lightweight updates in ClickHouse: they’re compact and efficient .

Visual walkthrough: how patch parts update rows #

Let’s visualize what happens during the update from above:

(We’ve simplified the real implementation slightly to focus on the core concept; we’ll walk through the full details in the next section.)

① Original data part : 
Contains both the keyboard and mouse orders, explicitly sorted by the table’s sorting key (order_id, item_id) . Each row also has a virtual _part_offset system column that reflects its position inside the part, so the part is implicitly sorted by _part_offset as well.

② Patch part : 
Created by the UPDATE. It contains only the changed values ( quantity , discount ) for the second row of the source part all_1_1_0 , plus metadata system columns: _part = all_1_1_0 , _part_offset = 1 .

③ Merged result : 
During a background merge, ClickHouse merges the original and patch part together, replacing matching rows with updated values.

They’re compact #

Patch parts minimize what’s written, by design:

Only the updated values are written.

Unchanged columns (e.g. order_id , item_id , price ) are skipped entirely; they don’t appear in the patch part at all. (With a specialized engine like ReplacingMergeTree, updates require re-inserting the full row, including unchanged values.)

They’re efficient #

Patch parts piggyback on merges already happening in the background , they hook into the process ClickHouse already runs continuously, with almost zero overhead:

All parts are naturally aligned : the original part is sorted by the table’s sorting key (and implicitly by _part_offset ), and the patch is sorted by (_part, _part_offset) .

That means merges can align rows seamlessly using _part_offset , without extra indexing, resorting, or rewrites.

Updates just fall into place , applied in a single, efficient merge pass: the engine simply interleaves the part’s data with a single linear scan of the parts , with no temp buffers or random access

(This mirrors the specialized engines, but patch parts rely on _part_offset instead of the sorting key.)

We’ve intentionally simplified things slightly to focus on how patch parts work in principle, on a single update.

Now let’s unfold the full picture and see how they scale.

Patch parts at scale: tracking, targeting, merging #

To understand how ClickHouse supports scalable, production-grade updates , we’ll go through the internal system columns and metadata structures that power patch parts.

Real-world updates rely on these internals to efficiently target rows across parts and support non-blocking updates at scale.

Let’s zoom in.

Suppose we want to apply a 20% discount to all items in all orders where quantity is 40 or more:

1 UPDATE orders 2 SET discount = 0.2 3 WHERE quantity >= 40 ;

This kind of SQL statement can affect many rows across many parts . But you don’t need to worry about how. It just works.

Declarative updates matter 
Unlike specialized engines like ReplacingMergeTree, where you’d need to insert a new row per updated row, declarative SQL updates handle the mechanics for you. Just express your intent.

To show how patch parts efficiently target rows across multiple parts , the diagram below uses an extended example with more inserts and the update above.

Note: To keep things focused, we dim all system columns and structures not used in this example .

• Orange highlights system columns that are used. 
• Blue highlights indexes that are used. 
• Light gray indicates unused structures.

This lets us highlight the essentials while still giving you a full view of what exists under the hood.

① Initial order : Our base example with two items.

② Another order : A new order_id , inserted separately.

③ Extended order : Adds two more items to the initial order.

These are just regular inserts, resulting in three data parts created over time.

④ Patch part : Created by the UPDATE. Contains just the new column values (discount = 0.2 for two rows) and enough metadata to target the exact rows being changed

⑤ Merged data part : Combines the old rows and patch values into one output part

Cleanup: After the merge, the original parts ① to ③ (and eventually also ④) are dropped. Only ⑤, the merged result, remains.

Now let’s walk through the system columns and internal data structures sketched on the diagram above.

System columns in original rows #

To enable patch part mechanics, every row in an original data part carries three system columns : 
System column(s) Description and purpose _part_offset The row’s ordinal position within its part; used to align rows during merges and apply patch updates efficiently. _block_number , _block_offset The insert-time block number and row’s offset within that block; used to locate the row after merges when _part_offset is no longer valid. ( not used in this example ) 
(These system columns are virtual by default. Like in the earlier simplified example, we show them materialized here for clarity; in reality, they’re only stored physically after a merge.)

Precise targeting metadata in patch parts #
-----

Task: Extract ONLY content relevant to this goal:
GOAL: Write an operator-facing HOWTO for ClickHouse SQL-style UPDATE/DELETE (patch parts / lightweight updates). Include: prerequisites/compatibility, enabling flags/settings, safe rollout checklist, 2–4 minimal example SQL commands, verification steps/queries, gotchas/limits. Exclude design rationale, benchmarks, images, proofs, marketing. Use concise bullets and headings.

Output format (markdown; keep it concise):
- Prefer actionable “how-to” material over explanations of design decisions.
- Include concrete settings/flags and their values when mentioned.
- Include short example SQL snippets (fenced code blocks) if the chunk contains enough detail.
- Include 2–6 “gotchas/limits/compatibility” bullets if mentioned.
- Avoid: deep internal design rationale, pictures, proofs, marketing fluff.

Output structure:
1) `## Practical notes` (bullets)
2) `## Settings & knobs` (bullets, `name = value` when possible)
3) `## Examples` (0–2 fenced SQL blocks + 1–3 bullets)
4) `## Gotchas` (bullets)
5) `## Verification` (0–1 fenced SQL block + 2–6 bullets)

mcp: gh-demo starting
mcp: ai-demo starting
mcp: gh-mcp starting
mcp: gh-demo ready
mcp: gh-mcp ready
mcp: ai-demo ready
mcp startup: ready: gh-demo, gh-mcp, ai-demo
codex
## Practical notes
- SQL `UPDATE` using patch parts is called a “lightweight update”.
- In ClickHouse `25.7`, lightweight updates are **experimental**.
- Updated values become **visible to queries immediately** (no need to wait for background merges).
- Patch parts store only changed column values plus targeting metadata, and are later applied during background merges.

## Settings & knobs
- `allow_experimental_lightweight_update = 1`
- Table setting changes:
  - `enable_block_number_column = 1`
  - `enable_block_offset_column = 1`
- Note: Expected to enter **beta** in `25.8`.

## Examples
```sql
SET allow_experimental_lightweight_update = 1;

ALTER TABLE orders
  MODIFY SETTING enable_block_number_column = 1, enable_block_offset_column = 1;

UPDATE orders
SET quantity = 60, discount = 0.20
WHERE order_id = 1001 AND item_id = 'mouse';
```

```sql
UPDATE orders
SET discount = 0.2
WHERE quantity >= 40;
```
- Works with standard SQL `UPDATE` syntax.

## Gotchas
- `25.7`: lightweight `UPDATE` is **experimental** and must be explicitly enabled.
- Requires enabling extra table settings (`enable_block_number_column`, `enable_block_offset_column`) before relying on patch-part updates.

## Verification
```sql
SELECT *
FROM orders
WHERE order_id = 1001 AND item_id = 'mouse';
```
- Verify the row reflects new values right after `UPDATE` (immediate visibility claim).
- For bulk updates (e.g., `quantity >= 40`), spot-check a few qualifying rows to confirm `discount = 0.2`.
- Confirm settings were applied: `SET allow_experimental_lightweight_update = 1` and the table has the required settings enabled (mechanism/queries not provided in this chunk).
tokens used
10,708
## Practical notes
- SQL `UPDATE` using patch parts is called a “lightweight update”.
- In ClickHouse `25.7`, lightweight updates are **experimental**.
- Updated values become **visible to queries immediately** (no need to wait for background merges).
- Patch parts store only changed column values plus targeting metadata, and are later applied during background merges.

## Settings & knobs
- `allow_experimental_lightweight_update = 1`
- Table setting changes:
  - `enable_block_number_column = 1`
  - `enable_block_offset_column = 1`
- Note: Expected to enter **beta** in `25.8`.

## Examples
```sql
SET allow_experimental_lightweight_update = 1;

ALTER TABLE orders
  MODIFY SETTING enable_block_number_column = 1, enable_block_offset_column = 1;

UPDATE orders
SET quantity = 60, discount = 0.20
WHERE order_id = 1001 AND item_id = 'mouse';
```

```sql
UPDATE orders
SET discount = 0.2
WHERE quantity >= 40;
```
- Works with standard SQL `UPDATE` syntax.

## Gotchas
- `25.7`: lightweight `UPDATE` is **experimental** and must be explicitly enabled.
- Requires enabling extra table settings (`enable_block_number_column`, `enable_block_offset_column`) before relying on patch-part updates.

## Verification
```sql
SELECT *
FROM orders
WHERE order_id = 1001 AND item_id = 'mouse';
```
- Verify the row reflects new values right after `UPDATE` (immediate visibility claim).
- For bulk updates (e.g., `quantity >= 40`), spot-check a few qualifying rows to confirm `discount = 0.2`.
- Confirm settings were applied: `SET allow_experimental_lightweight_update = 1` and the table has the required settings enabled (mechanism/queries not provided in this chunk).
