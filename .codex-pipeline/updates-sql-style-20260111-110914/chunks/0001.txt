TL;DR

We reimagined SQL-style UPDATEs from the ground up for ClickHouse’s column store. In this post, we’ll walk through how we did it, from heavyweight mutations to lightweight updates powered by patch parts that scale. Benchmarks are in Part 3.

This post is part of our series on fast UPDATEs in ClickHouse: 
Part 1: Purpose-built engines 
Learn how ClickHouse sidesteps slow row-level updates using insert-based engines like ReplacingMergeTree, CollapsingMergeTree, and CoalescingMergeTree.

This part: Declarative SQL-style UPDATEs 
Explore how we brought standard UPDATE syntax to ClickHouse with minimal overhead using patch parts.

Part 3: Benchmarks 
See how fast it really is. We benchmarked every approach, including declarative UPDATEs, and got up to 1,000× speedups .

Bonus: ClickHouse vs PostgreSQL 
We put ClickHouse’s new SQL UPDATEs head-to-head with PostgreSQL on identical hardware and data—parity on point updates, up to 4,000× faster on bulk changes.

The update they said you couldn’t have #

Column stores weren’t supposed to have fast updates.

For years, systems built for analytics sacrificed update performance for read speed. Row-level changes were considered incompatible with high-throughput, scan-optimized architectures.

ClickHouse was no exception, until we broke that rule by rethinking what an update really is .

In Part 1 , we showed how ClickHouse embraced a radically different model: turning updates into inserts. With purpose-built engines like ReplacingMergeTree and CollapsingMergeTree, we let merges resolve updates later, asynchronously, while maintaining blazing ingest speeds.

But not everyone wants to think in merge semantics. Many users just want to write:

1 UPDATE orders 2 SET discount = 0.2 3 WHERE quantity >= 40 ;

So we set out to make that possible, without losing what makes ClickHouse fast.

This post is about how we did it.

We’ll walk through the evolution of SQL-style updates in ClickHouse:

Classic mutations , simple but heavyweight.

A step forward: on-the-fly updates that avoided waiting for mutations to finish.

Finally closing the loop with fast, declarative SQL updates, powered by patch parts , a scalable, columnar-native update mechanism purpose-built for high-frequency workloads.

Curious how fast patch-part updates really are? 
In Part 3 , we benchmarked them, and saw speedups of up to 1,000×, sometimes even 1,600× or more.

Let’s first look at the original mutation-based UPDATEs that ClickHouse has supported for years.

Stage 1: Classic mutations and column rewrites #

Since 2018, ClickHouse has supported SQL-style updates using ALTER TABLE ... UPDATE statements.

We’ll use the same orders table from our example in Part 1 to illustrate how UPDATEs work via mutations:

1 CREATE TABLE orders ( 2 order_id Int32, 3 item_id String, 4 quantity UInt32, 5 price Decimal ( 10 , 2 ), 6 discount Decimal ( 5 , 2 ) 7 ) 8 ENGINE = MergeTree 9 ORDER BY (order_id, item_id);

Let’s walk through how a simple UPDATE triggers a mutation behind the scenes, starting with an initial insert.

Initial insert #

We start with a part containing two items from the same order:

1 INSERT INTO orders VALUES 2 ( 1001 , 'kbd ', 10 , 45.00 , 0.00 ), 3 ( 1001 , 'mouse ', 6 , 25.00 , 0.00 );

This creates a data part named all_1_1_0 :

UPDATE triggers a mutation #

We update the quantity and discount for the mouse item:

1 ALTER TABLE orders 2 UPDATE quantity = 60 , discount = 0.20 3 WHERE order_id = 1001 AND item_id = 'mouse ';

The ALTER TABLE ... UPDATE syntax differs intentionally from standard SQL UPDATE to reflect what’s happening under the hood: instead of modifying rows in place, ClickHouse rewrites (“mutates”) data parts.

Because of the UPDATE, ClickHouse now runs a mutation behind the scenes. This triggers three internal steps:

A new block number is allocated for the update (e.g. 2 ), which ClickHouse uses to track which parts need rewriting.

A new mutated part is created on disk, named all_1_1_0_2 , where 2 is the mutation version .

The mutation is applied only to parts with a block number less than 2 like the original part all_1_1_0 .

The diagram below shows what changes: the updated columns ( quantity , discount ) are fully rewritten , and the unchanged columns ( order_id , item_id , price ) are hard linked . No data is copied for those columns, the new part simply reuses the same underlying files via hard links on disk:

Once the mutation completes, the mutated part all_1_1_0_2 replaces the original all_1_1_0 , and the original part is removed. Although this deletes the original folder and its file entries, any column files that the new part all_1_1_0_2 hard links to remain safely on disk, because hard links ensure the data isn’t deleted until no part references it.

Summary and tradeoffs #

The classic mutation model is reliable, but comes with tradeoffs:

Heavyweight updates : Every UPDATE rewrites the affected columns, which can be costly at scale.

Delayed visibility : Changes don’t appear in query results until the background mutation completes.

Merge dependency : Mutations must wait for prior merges and mutations to finish before they run. (We haven’t covered this behavior in detail here, but it’s a real and sometimes surprising constraint.)

By default , ALTER TABLE … UPDATE statements run asynchronously, allowing ClickHouse to fuse multiple updates into a single mutation. This helps amortize the rewrite cost.

Before we move on to on-the-fly mutations, let’s pause to look at one more stopgap optimization built on this model: lightweight deletes .

Stage 1.5: Lightweight DELETEs (still mutations, but faster) #

Before on-the-fly mutations, ClickHouse introduced a simpler optimization to make DELETE faster under the classic mutation model.

Instead of removing rows immediately, DELETE is rewritten as an ALTER TABLE ... UPDATE that sets a special mask column _row_exists = 0 . This triggers a lightweight mutation that (re)writes* only the _row_exists column, all other columns are hard-linked , avoiding unnecessary I/O.

* If this is the first delete mutation on a part, the _row_exists column is created in the mutated part. If it’s a subsequent delete mutation (before the part has been merged), _row_exists is rewritten .

The next diagram shows the original part and the mutated result of a lightweight DELETE . Below, we walk through each step in the process: 
(For clarity, we show _row_exists in the original part, but as explained earlier, that column doesn’t exist yet and is added by the first delete mutation.)

1. Initial insert : We reuse the same small orders example from Stage 1: two rows inserted, creating the original part all_1_1_0 :

1 INSERT INTO orders VALUES 2 ( 1001 , 'kbd ', 10 , 45.00 , 0.00 ), 3 ( 1001 , 'mouse ', 6 , 25.00 , 0.00 );

DELETE issued : The mouse item is deleted:

1 DELETE FROM orders WHERE order_id = 1001 AND item_id = 'mouse ';

Internally, ClickHouse updates _row_exists = 0 :

1 ALTER TABLE orders 2 UPDATE _row_exists = 0 3 WHERE order_id = 1001 AND item_id = 'mouse ';

Mutation created : A new part all_1_1_0_2 is created, rewriting only _row_exists .

Query behavior : Rows where _row_exists = 0 are excluded from results.

Cleanup : The row is permanently dropped during the next regular background merge.

Summary and tradeoffs #

This approach made DELETE significantly faster without changing the core mutation model, but it still required background rewrites.

The next stage didn’t eliminate background rewrites, but it made updates visible immediately, without waiting for the mutation to run.

Stage 2: On-the-fly updates for instant visibility #

Classic mutations are heavyweight: they rewrite full data columns and take time to finish, especially on large datasets. To reduce the latency between issuing an UPDATE and seeing the result, ClickHouse introduced on-the-fly mutations , an optimization that makes updates visible immediately, even before any part is rewritten.

This was a natural first step on the path to patch parts. It didn’t avoid rewrites, but it made updates feel instant.

The diagram below illustrates the mechanism: the UPDATE is stored in memory and applied on read, while the actual mutation runs asynchronously in the background:

Insert : As before, two rows are ① inserted into the orders table, ② creating the initial part all_1_1_0 .

UPDATE issued : We ③ update the mouse row. ClickHouse stores the update expression in memory.

SELECT issued : The ④ query reads the original data part, but ClickHouse ⑤ applies the update on the fly in memory.

Result : The updated row is visible immediately in the ⑥ query result.

Mutation runs : In the background, ClickHouse ⑦ rewrites the part as all_1_1_0_2 and drops the old one.

Summary and tradeoffs #

On-the-fly mutations improve responsiveness, but they don’t eliminate background rewrites. They can also slow down SELECTs if many updates accumulate, and support for subqueries and non-determinism is limited .